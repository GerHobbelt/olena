I ARBORESCENCE
--------------

bench  ==> Comparaison d'algorithmes d'un point de vue temps d'exécution.
bug    ==> Bug rencontrés dans milena et archivés sous forme de programme.
build  ==> Répertoire d'exécution, non sauvé dans git.
demo   ==> Première version d'un algorithme pour voir son comportement.
doc    ==> Documentation tex ou code minimal pour de petits exemples.
exp    ==> Version avancée des algorithmes pour traitées les bases de données.
mln    ==> Partie mise en librairie milena des différents travaux.
tests  ==> Tests unitaires sur certains algorithmes.
tools  ==> Découpage de certains algorithmes pour mieux les tester séparément.
use    ==> Test de compilation, code minimal pour compiler un élément.

II COMPILATION
--------------

L'unité minimale de code choisie est le répertoire.
Donc aller dans le répertoire qui nous interesse,
par exemple, green/demo/annotating/hsv et lancé le make

#:~/git/olena/scribo/sandbox/green$cd demo/annotating/hsv
#:~/git/olena/scribo/sandbox/green/demo/annotating/hsv$ make -f Makefile.am

Cette opération créé dans build le répertoire de compilation
green/build/demo/annotating/hsv. Dans ce répertoire aura été copié un
Makefile et tous les fichiers qui ne sont pas des sources. Par
exemple, des fichiers de calibration comme gaussian.sh (pour vérifier
la mire du filtre de gaussienne) ou de la documentation à la sauvette
sous forme de fichiers textes jetés à la va vite dans le répertoire
pour ne pas perdre l'information recherchée. En l'occurence, ici, il
n'y a rien à copier. Rendons-nous dans le répertoire de compilation et lançons
le makefile.

#:~/git/olena/scribo/sandbox/green/demo/annotating/hsv$
#:cd ../../../build/demo/annotating/hsv
#:~/git/olena/scribo/sandbox/green/build/demo/annotating/hsv$ make clean all

L'exécutable est généré par le makefile, il porte le nom du
répertoire.  Si il y a besoin de mettre à jour le makefile, le faire
dans le répertoire des sources en éditant Makefile.am, puis en
régénérant le Makefile dans le répertoire d'exécution par la commande
make -f Makefile.am depuis le répertoire source.


III MAKEFILE
------------

Les makefiles utilisés sont tous les mêmes avec quelques variables
dont le contenu change dans leur partie en-tête.

Pour chaque répertoire, le makefile doit savoir si le chemin courant
est un répertoire de compilation ou un répertoire de source. Pour les
identifier, il a recours à un pattern qui malheureusemnt fait
intervenir le nom de la branche de développement (bench,demo,bug,exp ...).

SOURCE_PATTERN= green/demo
BUILD__PATTERN= green/build/demo

Si un makefile ne fonctionne pas, il faut vérifier ceci en premier
lieu. Ici, le makefile doit être situé dans la branche démo.

Autre élément à savoir, la compilation nécessite d'inclure la
librairie milena, ainsi que les développements propres en vu de leur
intégration futur dans milena, ceci est fait par un jeu d'INCLUDES1
et INCLUDES2.

INCLUDES1= -I$(HOME)/git/olena/scribo/sandbox/green
INCLUDES2= -I$(HOME)/git/olena/milena
INCLUDES=  $(INCLUDES1) $(INCLUDES2)

Suivant l'allure du compte où l'on exécute les makefiles, il faut
revoir le chemin pour trouver des deux répertoires.

Enfin, les options de compilations ne sont pas toujours les mêmes. Les
trois lignes possibles sont toutes présentes et seule celle qui est
utilisée n'est pas commentée. Typiquement, dans la branche de
développement démo où les perfomances ne sont pas le problème, on
compilera avec tout le matériel pour utiliser gdb et sans
optimisation. A l'inverse, dans la branche d'expérimentation, où le
code a déjà été testé, on cherche à aller vite car on exécute ce code
sur de nombreuses images. Dans cette optique, pas de débugage, pas de
traçage, optimisation conséquente.

CXXFLAGS= -ggdb -O0 -Wall -W -pedantic -ansi -pipe $(INCLUDES)
#CXXFLAGS= -DNDEBUG -O1 -Wall -W -pedantic -ansi -pipe $(INCLUDES)
#CXXFLAGS= -DNDEBUG -O3 -Wall -W -pedantic -ansi -pipe $(INCLUDES)

Une dernière dernière information, dans le cadre des développements
exp, et tools, on utilise la librairie boost soit pour la
virtualisation du filesystem, soit pour le formatage des fichiers text
(réalisation de colonnes, mettre des entiers sur un certain nombre de
caractères). Une ligne de chargement des librairies peut apparaitre donc.

LOADLIBES= -lboost_filesystem

On retrouvera les includes suivantes dans les sources:

#include <boost/format.hpp>
#include <boost/filesystem.hpp>


IV CHEMINS DES IMAGES
---------------------

Toutes les images ont toujours été locales sur mon ordinateur. La
politique a toujours été d'utiliser un fichier img_path pour coder les
chemins des images.  Les chemins étant plutôt long, j'ai toujours eu
tendance à faire en sorte qu'ils soient compilés en dur (sauf pour la
partie développement tools qui est vraiment voué à donner des
exécutables indépendants et génériques). Le fichier mln/img_path.hh
code la position de toutes les images dans mon arborescence. Il faudra
donc veiller à changer tous les chemins pour les adapter au compte
dans lequel on voudra reprendre le code. Dans le code, les références
aux positions des images sont faites via des macros.

Toutes les images sont située dans git/img. En règle générale, je ne
traite que des images au format .pgm, .pbm et .ppm. Il m'arrive
fréquemment de dumper des images au format .sh (gnuplot shell
image). Pour la branche tools, nous avons utilisé les dumps de milena
comme format de transfert d'un utilitaire à un autre. Les images sont
classées suivant leur provenance. Nous avons tout d'abord la base
OLENA (copie des images de tests milena), la base INIM (très peu
utilisée voire jamais), la base ICDAR (très utilisée, surtout dans
exp), la base AFP (très utilisée dans exp) et les bases ANNOTATING1 et
ANNOTATING2 (pas très utilisées ni l'une, ni l'autre).

La plus part du temps, sauver les résultats dans le répertoire
d'exécution courant est largement suffisant. Parfois, il est
nécessaire de sauvegarder de grosses quantités d'informations et de
les classer comme dans la branche de développement exp. C'est pour
cela, qu'un certain nombre de macros définissent des endroits pour
sauvegarder les résultats lors d'expérimentation de grande ampleur sur
toute la base ICDAR ou AFP.


V GNUPLOT SCRIPT SHELL IMAGE FORMAT
-----------------------------------

J'abrège le nom du format par gnuplot shell format. En fait, c'est un
format d'image particulier qui a besoin de gnuplot pour être lu. Il
est donc compatible avec aucun viewer si ce n'est gnuplot, mais a la
caractéristique d'afficher tous les points de manière visible. Par
ailleurs, comme il s'agit d'un script gnuplot il permet d'insérer très
facilement une fonction pour visualiser les données autrement (par
exemple, changer d'espace: HSL, HSV). Le fichier tire son nom de la
façon dont il fonctionne. C'est un script shell qui fait un appel à
gnuplot et lui passe le jeu de données directement à partir de ce même
fichier, pas besoin de faire appel à un autre fichier. Une fois
généré, le fichier doit être modifié pour avoir les permissions
d'exécution (chmod 755 gnuplot_shell_file.sh). Comme je trouve le format
extrêmement pratique, il se retrouve preque partout dans mes sources.


VI HISTOGRAMMES
---------------

Un des travaux demandés par théo est la réalisation d'une librairie
d'histogramme permettant de fournir un résultat sous forme d'image.
L'intérêt est ensuite de pouvoir filtrer directement ces histogrammes
par les algorithmes de milena, ou encore d'étudier les valeurs
caractéristiques par d'autres accumulateurs. Les codes réellement
utilisés sont histo1d et histo3d RGB. Tous les autres codes sont très
expérimentaux. Notemment, le code HSL demande de quantifier l'espace
de comptage puisqu'il est décrit sous la forme de triplets de float
(les autres sont inférés). Néanmoins, le code est à conserver car il
contient une séquence d'appels pour les routines permettant de
considérer la dimension de la teinte comme circulaire.

Après réflexion, le code des histogrammes et tous les accumulateurs
qui en découlent devraient être rangés dans l'espace de nommage
mln::accu::histo. Cela permettrait de ne pas parasiter mln::accu::stat
avec tous les éléments propres aux histogrammes et de ne pas faire un
espace de nommage à rallonge en introduisant histo dans stat. Donc
mln::accu::stat semble être une bonne postion pour le rangement final
du code relatif aux histogrammes.


a) version 1d

* mln/accu/stat/histo1d.hh: Accumulateur histogramme image1d.
* use/accu/stat/histo1d: Code minimal utilisant un histogramme 1d.
* tests/accu/stat/histo1d: Tests unitaires sur l'histogramme 1d.


b) version 2d

* mln/value/rg.hh: Définition du type vectoriel 2d rouge/vert (RG).
* use/value/rg: Exemple de code pour l'utilisation de rg.

* mln/fun/v2v/rgb_to_rg.hh: Transformation de l'espace RGB vers l'espace RG.
* use/fun/v2v/rgb_to_rg: Exemple de code pour l'utilisation de rgb_to_rg.

* mln/accu/stat/histo2d.hh: Accumulateur histogramme image2d.
* use/accu/stat/histo2d: Code minimal utilisant un histogramme 2d.


c) version 3d RGB


* mln/fun/v2v/rgb8_to_rgbn.hh: Diminution de la quantification (n < 8 bits).
* use/fun/v2v/rgb8_to_rgbn: Exemple de code pour l'utilisation de rgb8_to_rgbn.

* mln/accu/stat/histo3d_rgb.hh: Accumulateur histogramme image3d RGB.
* use/accu/stat/histo3_rgb: Code minimal utilisant un histogramme 3d RGB.


d) version 3d HSL

* mln/accu/stat/histo3d_hsl.hh: Accumulateur histogramme image3d HSL.
* use/accu/stat/histo3_hsl: Code minimal utilisant un histogramme 3d HSL.
* tests/accu/stat/histo3d_hsl: Tests unitaires sur l'histogramme HSL 3d.

Le code HSL ne compile plus car l'interface liant la transformation du
domaine et la fonction fold a changée. Je n'ai pas le temps de
regarder plus avant.


VII SAUVEGARDE FORMAT GNUPLOT SHELL
-----------------------------------

Ce travail est personnel. Ces développements m'ont été tellement
utiles que je ne regrette pas l'investissement effectué. L'idée est de
pouvoir avoir un format d'image en écriture lisible. J'ai pris celui
de gnuplot, ce qui permet en plus de pouvoir "sucrer" la présentation
des données à loisir. Les images sont plus lourdes car le codage est
textuel et un peu plus "verbose" mais se compresse aisément par
n'importe quel algorithme type huffman (tous les archiveurs en possède un).

* mln/io/plot/save_image_sh.hh: Librairie de sauvegarde au format gnuplot shell.
* use/io/plot/save_image_sh: Code simple d'utilisation de la sauvegarde.
* tests/io/plot/save_image_sh: Tests unitaires sur l'export.



VIII VISUALISATION HISTOGRAMMES 3D
----------------------------------

Les histogrammes sur lesquels nous sommes amenés à travailler sont en
3d.  Nous étudions des images couleurs dans l'espace RGB, donc
l'histogramme dans cet espace possède 3 dimensions (une par axe, R, G
et B) et la cellule désignée par cette position RGB renfermera le nombre
de pixels de l'image ayant exactement cette couleur. Il est très
difficile de bien visualiser ces images, car elles sont en 3d. Gnuplot
nous donne qu'une aide très limitée, car il faut afficher 3 axes et la
force de la cellule, ce qui fait en fait quatre informations à
afficher. Il est possible de jouer avec les couleurs, mais il n'y a
pas de représentation très utile, on peut afficher le nuage des
couleurs pour percevoir sa forme générale, mais l'interprétation ne
peut pas vraiment aller plus loin. C'est pourquoi, nous proposons une
représentation plus pertinente. C'est Théo qui a roulé sa bosse en
classification qui a developpé ce genre de technique, je ne fais que
réutilliser les mêmes analyses en beaucoup moins fins que ce qu'il a
déjà pu développer par ailleurs.

La première idée est de gommer une dimension, et c'est possible de le
faire car dans une image RGB, souvent l'axe bleu est fortement
correllé avec l'axe vert. Cela dépend beaucoup de la formation des
images et pour des images synthétiques, il n'y a aucune raison que
ce soit le cas. C'est juste une propriété qui s'applique pour les
images naturelles. La forte corrélation obtenue ne veut pas dire qu'il
y a exactement les mêmes informations, juste que dans l'ensemble on
note une relative redondance (du point de vue statistique). Pour t'en
convaincre, tu peux regarder sous gimp, les histogrammes sur les
différents canaux et tu observeras qu'il y a deux canaux (le vert et
bleu) pour lesquels les histogrammes sont presque identiques.

La dernière étape est de réaliser un traitement sur la dimension qui a
été supprimée. Par exemple, sommer toutes les valeurs rencontrées ou
ne retenir que la valeur max ...

Le fichier display_histo.hh joue le rôle de front hand pour les
routines de visualisation. Il a pour rôle d'initialiser, d'instancier,
de coordonner la chaine de visualisation. Le fichier project_histo.hh
décrit les algorithmes au coeur de la visualisation, comment est
construite concrêtement l'information que l'on visualise à la fin.

Nous avons construit 3 sortes de projecteurs. Le premier va sommer le
long de l'axe bleu les différentes informations
rencontrées. Globalement, cela signifie, que plus il y a de pixels
dans le sous espace (red=v1,green=v2) avec pour chaque pixel
rencontrés des valeurs de bleu différentes, plus le nombre construit
sera grand. Sur cette information, on fera passer un logarithme pour
diminuer les pics et ne faire ressortir visuellement que les
informations les plus saillantes. En dernier lieu, on rescalera les
valeurs obtenues pour utiliser pleinement la dynamique d'une image de
256 niveaux de gris. La visualisation obtenue nous donne un aperçu de
la densité de l'espace des couleurs. On observera en très lumineux les
endroits de forte population de couleurs et en presque noir l'absence
de couleur dans l'image pour cette portion de l'espace RGB.

Les autres techniques de visualisation sont des variantes du même
traitement que nous allons decrire. La chaine de base consiste à
parcourir la dimension bleu pour chaque point de l'espace rouge/vert
(comme dans la première technique). Cette fois-ci, ce n'est plus la
somme qui nous intéresse, mais la couleur la plus représentée. On va
faire un max des valeurs obtenues en parcourant l'axe bleu. Une
première version rapportera la position bleue qui rassemble le plus de
pixels. Une seconde version remplacera cette position bleue par un
label associé à une segmentation déjà effectuée dans cet espace.

La dernière technique modifie la version précédente de la manière
suivante: Nous sommes toujours interessé par le max, mais on va remplacer
cette information directement par la couleur reconstituée (nous sommes
sur un point particulier de l'espace red/green donc red et green sont
connus, et on vient d'obtenir la valeur bleue la plus représentée, on
utilisera cette valeur pour former une couleur RGB). A l'issu du
processus, on obtient donc une image couleur. L'information obtenue
ici, n'est plus la densité de l'histogramme, mais plutôt la couleur la
plus représentée à cet endroit. La toute première technique et celle-ci
sont complémentaires pour la bonne visualisation d'un histogramme
3d. Si l'on dispose d'une segmentation de l'espace des couleurs, on
peut trouver le label associé au bleu prédominant et récupérer dans
une palette de couleur annexe la couleur associée au label.

* mln/display/dispay_histo.hh: Front hand pour les routines de projection.
* mln/display/project_histo.hh: Les algorithmes de visualisations.

Le travail dans use ne représente que deux routines sur six de
display_histo.  Par manque de temps, je ne fais pas le reste de
manière à insister sur d'autres points documentaires plus
importants. Le cadre est en place et les autres routines plus
sophistiquées faisant intervenir la segmentation de l'histogramme
peuvent être tirées de tools/labeling/regmax par exemple.

* use/display/display_histo: Visualisations des histogrammes.


IX KMEANS
---------

Ce travail m'avait été demandé par théo. Je le laisse inachevé, quelque part
perdu pendant l'optimisation du code et sa transformation en canevas.


a) Première implémentation avec matrices et vecteurs

Cette version est bien documentée et permet de mieux comprendre les autres
versions. Par ailleurs, elle n'est pas spécialement optimisée ce qui fait
qu'elle colle davantage au modèle de l'algorithme kmean traditionnel.

* mln/clustering/k_mean.hh: Première implémentation avec matrices et vecteurs.
* use/clustering/k_mean:  Code minimal utilisant cette première implémentation.
* tests/clustering/k_mean: Tests unitaires sur la permière version.


b) Seconde implémentation avec image en 1d

Cette seconde version intègre une optimisation testée par théo il y a
longtemps. Je me demande si ce n'étais pas pendant sa thèse qu'il
avait travaillé sur cette version. Bref, dans la mesure où
l'optimisation passe par la création d'un histogramme, cette version
devient dépendante des histogrammes réalisés plutôt (il est aussi
dépendant de la sauvegarde au format gnuplot shell). L'idée générale
de l'optimisation est de ne plus raisonner sur l'image, mais sur
l'histogramme. Car à un moment donné, la classification ne tient
compte que de la valeur du pixel en intensité (pas de ses coordonnées
dans l'image). Donc tout pixel de même intensité sera classé de
manière identique. D'où l'intérêt de travailler avec les
histogrammes. Les limites de cette optimisation se trouvent dans la
taille des histogrammes. L'optimisation marche à merveille pour de
faibles quantifications (8 bits c'est parfait). Par contre, lorsque
l'on passe à un histogramme couleur, la taille de l'histogramme
devient problématique et peut dépasser la taille de l'image. Cette
optimisation n'a alors plus aucun sens.

* mln/clustering/kmean1d.hh: Implémentation 1d avec des images.
* use/clustering/kmean1d : Code minimal utilisant cette seconde implémentation.
* demo/clustering/kmean1d : Demonstrateur.

La visualisation de la convergence des moyennes, générée par le
démonstrateur, n'est pas très lisible. La lecture textuelle du fichier
gnuplot shell (mean_cnv.sh) permet d'interpréter ce qui se passe, par
contre le graphique est à refaire.


c) kmean2d

Cette troisième version est identique à la seconde, sauf qu'elle
permet de faire la classification dans un espace à deux
dimensions. Malheureusement, un tel travail dans cet espace coûte
beaucoup plus cher à l'exécution.

* mln/fun/v2v/rg_to_rgb.hh: Transformation de l'espace RG vers l'espace RGB.
* use/fun/v2v/rg_to_rgb: Exemple de code pour l'utilisation de rg_to_rgb.

* mln/clustering/kmean2d.hh: Implémentation 2d avec des images.
* use/clustering/kmean2d : Code minimal utilisant cette seconde implémentation.
* demo/clustering/kmean2d : Demonstrateur.

La visualisation de la convergence des moyennes est cette fois-ci
complètement aboutie. Le fichier semble avoir quelques soucis, il
manque des lignes dans le splot initial, en remettre pour qu'il y en
ait autant que les blocs de données. L'espace des couleurs étant le
bicanal r/g, on peut visualiser sur une troisième dimension
l'évolution des centres dans l'espace initial. L'effet est très réussi
et aussi très parlant. Chaque run, correspond alors à une trajectoire
particulière. L'affichage des variations des variances a été rénové et
est lui aussi beaucoup plus lisible.


d) kmean3d

Cette quatrième version est la copie conforme de la version
précédente. Les problèmes de visualisation sur les fichiés générés
sont les mêmes. L'affichage des convergences est identique. Le recours
à une dégradation de l'image d'entrée est impérative pour avoir des
temps encore acceptables.

* mln/clustering/kmean3d.hh: Implémentation 3d avec des images.
* use/clustering/kmean3d : Code minimal utilisant cette quatrième impl.
* demo/clustering/kmean3d : Demonstrateur.


e) kmean aplati

Cette cinquième version est très spéciale. Elle doit permettre de
gagner du temps d'exécution. Le concept est simple au départ, tout
écrire d'un seul tenant. L'exercice est fastidieux et difficile
(intellectuellement). Une fois fait, il faut réfléchir à ce qui peut
être mis sous forme de canevas. Par exemple, la manière de calculer
les distances peut être une information paramétrable. Cela pemettrait
de faire un benchmark in situ. La transcription actuelle compile. Elle
n'intègre pas tous les outils de debuggage que nous pouvions avoir sur
les autres versions. Par ailleurs, comme le code est réuni en une
seule fonction, nous avons pour l'instant une seule sortie, l'image de
labels de la classification réalisée. A partir de cette image, nous
pouvons en reconstruire d'autres. Il manque quand même la possibilité
d'observer les convergences. Le travail était en cours, à prendre donc
avec beaucoup de pincettes. La dernière exécution house.ppm, 3
centres, 10 itérations et 10 runs fonctionne parfaitement sur les
premiers runs puis l'affichage s'emballe sans que l'on y comprenne
rien. Un dump dans un fichier révèle le non appariement de certaines
traces (entering/exiting). Une piste à suivre est la sortie anticipée
d'une de mes boucles sans pour autant fermer une trace ... ???

* mln/clustering/kmean_rgb.hh: Implémentation 3d avec des images.
* use/clustering/kmean_rgb : Code minimal utilisant cette quatrième impl.
* demo/clustering/kmean_rgb: Utilisation de la version aplatie.


f) optimisation possible

Le calcul des distances entre les points et les différents centres
peut être réalisé par des transformées. Certes, les distances ne seront
pas les mêmes que la distance euclidienne, mais elles s'en approchent
et cela constitue très certainement une très bonne approximation pour
l'objectif que nous cherchons à atteindre. Le but de ce benchmark est
de regarder quel type de transformation est le plus rapide pour
arriver à nos fins en fonction des données d'entrée. Cette
optimisation n'a pas encore été intégrée dans le code, et reste une
piste à exploiter.

* bench/clustering/distance: Comparaison algorithmes d'évaluation des distances.

Une routine du benchmark ne compile plus. Il semble qu'il y ait un
mauvais appel à la fonction at_ dans la routine
influence_zone_geodesic.hh ligne 127. Le but du benchmark est de
tester les distances en 2d et 3d pour des voisinages différents (c04,
c08, c06, c18, c26) sur les routines distance euclidienne classique,
zone d'influence geodesique et zone d'influence "front". La première
serie de test vise à garder une taille d' image constante et
d'augmenter le nombre de centres pour voir comment se comporte les
algorithmes et la seconde serie, vise à garder un nombre constant de
centres et à agrandir progressivement l'image. Attention, le benchmark
essaye d'être assez exhaustif et donc se paye par un temps d'execution
assez long. Les différents fichiers générés reprennent les différents
tests effectués et montrent l'évolution du temps de calcul suivant la
progression du paramètre observé (taille de l'image ou nombre de
centres).


X REGIONAL MAXIMA
-----------------

Pour cette partie, le temps a commencé à s'accélérer, ce qui fait
qu'il n'y a plus de tests unitaires. Par ailleurs, comme la
fonctionnalité n'a pas été pensée comme un élément de la librairie, il
n'y a donc pas non plus de répertoire "use" à cet effet.


a) Segmentation basé sur le watershed

Cet exemple essaye une première méthode de filtrage de
l'histogramme. A noter elle ne compile pas. Elle eu compilé dans le
temps, mais le source a été tellement remanié que sa reconstruction
doit régler encore quelques détails.  Sa compilation n'a pas vraiment
d'intérêt en soi, seule la démarche compte ici.  Le temps me manque et
je ne persisterais pas à essayer de le faire compiler à tout prix. D'autres
sources sont beaucoup plus intéressant. Nota, gaussian.sh est un gnuplot
script shell qui sert à calibrer le filtrage de l'histogramme. Il permet
de déterminer visuellement la taille de la fenêtre win utilisée lors de la
convolution en fonction d'un écart type sigma donné.

* demo/labeling/watershed: Demonstrator de la segmentation d'un histogramme.


b) Second essai avec les regional_maxima

Le programme actuel ne compile pas non plus. Il n'est pas d'une grande
importance, alors du coup je le laisse tel quel. Il a servi a
plusieurs choses, en premier à tester les regional_maxima mais en
dernier lieu surtout à tester le filtrage par attribut (volume) qui
semblait avoir quelques soucis.  A l'heure d'aujourd'hui, je ne peux
toujours pas dire s'il fonctionne convenablement ou non, il semble que
parfois des anomalies se produisent à ce niveau, mais elles peuvent
aussi bien être générées par un problème interne à mon programme.

* demo/labeling/regional_maxima: Testeur du filtrage par attribut de volume.


c) Déplacement du programme b) dans exp.

Même programme que b) mais qui compile.

* exp/labeling/regional_maxima: Testeur du filtrage par attribut de volume.


d) Sources importantes ==> regional_maxima

Il y avait une autre version des regional_maxima ou toute la chaine était
présente, si j'avais davantage de temps, je l'aurais réecrite. Il semble
qu'elle se soit perdue lors de la migration de SVN vers GIT. Aucune importance.
Les outils "up to date" sont ceux dont les sources sont dans tools. La classe
outil représente des binaires un peu plus travaillé au niveau de l'interface
texte, en gros ils prennent plein de paramètres et génèrent des résultats
intermédiaires. Chaque outil à son usage ...


Ce qu'il faut savoir sur l'outil histo. Tout d'abord le programme
prend 7 ou 8 arguments et c'est le dernier qui est optionnel. Les
arguments sont l'image couleur à partir de laquelle on veut extraire
l'histogramme couleur (3d), le degré de quantification que l'on veut
utiliser (sous échantillonnnage de l'image), le chemin de l'image
couleur quantifiée résultante après analyse, le chemin de
l'histogramme produit (réutilisé par les autres outils), la
visualisation rouge/verte de l'histogramme (brute) et la visualisation
augmentée de ce même histogramme avec les maxima dessus. Se reporter à
la documentation des histogrammes pour avoir plus d'information sur ce
sujet. Le dernier paramètre optionnel est un masque à appliquer sur
les pixels de l'image d'entrée. La chaine de traitement de l'outil,
commence par sous échantillonner l'image en n bits, n < 8, puis
construit l'histogramme 3d à partir des pixels présents dans le masque,
si ce dernier existe et enfin sauvegarde des résultats et des images
servant à la visualisation.

* tools/labeling/histo: Construction de l'histogramme.


Ce qu'il faut savoir sur l'outil de filtrage de l'histogramme. Tout
d'abord, il prend en paramètre 6 arguments obligatoires. Les arguments
sont la quantification utilisée par l'outil de création de
l'histogramme, le chemin de l'histogramme à filtrer, le seuil utilisé
pour le filtrage (c'est le volume minimal en dessous duquel
l'information est jugée comme du bruit et donc ne doit pas passer), le
chemin de l'histogramme filtré, le chemin de la projection en densité
de l'histogramme et le chemin de la projection en couleurs
majoritaires de l'histogramme. Se reporter au chapitre sur la
visualisation pour comprendre les différentes projections. Le filtrage
est un traitement très simple, il applique uniquement l'algorithme
morphologique d'ouverture sur les attributs de volume. Imagine un
histogramme en 1d, l'attribut de volume est alors la surface sous la
courbe de l'histogramme. Une ouverture volumique consiste à supprimer
tous les pics ayant un volume inférieur au seuil. Cela fonctionne très
bien en général, toutefois, il y a des cas où les informations
retournées ne sont pas celles attendues. Concrêtement, des composantes
de volume inférieur au seuil sont retrouvées plus loin dans la chaine
de traitement, ce qui contredit l'usage de l'ouverture
morphologique. Il est difficile de définir ce qui se passe vraiment,
il est possible qu'il y ait un bug ou simplement que le reste de ma
chaine induise quelque fois ce résultat étrange. Honnêtement, je ne
sais pas ce qui se passe. Le cas n'arrive pas fréquemment, donc il est
très difficile d'isoler l'erreur.

* tools/labeling/opening: Filtrage de l'histogramme.


Ce qu'il faut savoir sur l'outil de labellisation. Tout d'abord, il
prend 11 arguments dont le dernier est optionel. Les arguments sont
l'image originale (nécessaire pour calculer la couleur moyenne des
labels ==> réalité augmentée), la quantification utilisée
précédemment, l'image préalablement quantifiée, l'histogramme
original, l'histogramme filtré, le voisinage sur lequel effectué les
opérations (c6, c18 ou c26), l'histogramme segmenté en 3d, la
projection red/green associée, la colormap, l'image reconstruite des
moyennes associé à chaque label et éventuellement un fichier décrivant
les statistiques associés aux labels (couleur moyenne associé au
label, plus nombre de pixels concernés dans l'image, plus pourcentage
absolu et pourcentage sans tenir compte du fond). Le seul appel
intéressant est la labellisation de l'histogramme filtré qui produit
une image 3d de label. Tout le reste est de la tuyauterie pour faire
de la rélatité augmentée. Par exemple pour formée l'image des moyennes
associées à chaque label, il est nécessaire de construire l'image 2d
des labels qui fait intervenir elle-même l'image d'entrée quantifiée
et l'image originale. Idem pour la construction de la
colormap. L'outil segmente l'histogramme filtré par la détermination
des maxima régionnaux, autrement dit des zones de très fortes
densités. Ces zones sont en général très petites, quelques pixels pas
plus. C'est pourquoi l'outil suivant va nous servir à les étendre un
peu.

* tools/labeling/regmax: Segmentation de l'histogramme.


Petite suprise désagréable, l'outil iz ne compile plus. La raison en
est toute simple, il y a eu du mouvement dans les fonctions
mln::transform::influence_zone_geodesic(). Un revamp un peu violent,
avec une version
mln::transform::influence_zone_geodesic_saturated(). J'ai corrigé le
changement d'appel, mais cela ne suffit pas. Dans la méthode
mln::transform::influence_zone_geodesic_fastest(), l.127, tu utilises
la méthode at() pour une image 2d et là pas de chance, moi j'utilise
cet algorithme pour une image de labels 3d (nous sommes dans un espace
RGB quantifié en n bits, n < 8). Je ne veux surtout rien détruire dans
scribo, donc je te laisse effectuer les changements qui s'imposent et
je continue la documentation.

Ce qu'il faut savoir sur iz. Tout d'abord cette routine prend 12
arguments dont le dernier est optionel. Les arguments sont
l'histogramme labelisé par l'outil précédent, la profondeur utilisée
pour l'influence par zone géodésique (0 signifie l'infini), puis vient
le voisinage 3d utilisé pour la propagation, puis l'image initiale,
puis le degrée de quantification utilisé par tous les outils, puis le
dump de l'histogramme, la colormap (utilisée pour la projection r/g
avec segmentation, puis l'histogramme labelisé après propagation nommé
iz et enfin la projection r/g, l'image reconstruite à l'aide des
moyennes des classes et le fichier de statistiques intégrant la
propagation des classes. La propagation a pour vocation d'agrandir les
classes obtenues jusqu'à maintenant. C'est une information que l'on
voit bien avec les changements dans les projections r/g.

* tools/labeling/iz: Propagation des classes par zone d'influence.


Comme le paramétrage est difficile, il est conseillé de relire les
scripts des jeux de tests qui coordonnent l'appel de ces routines avec
une sucession de paramètres cohérents. Je sais ce que tu vas me dire,
c'est mal, il ne faut pas mettre d'images sur le dépot git. Dans
l'absolu, je suis d'accord, mais aujourd'hui, le code iz est cassé et
rejouer ces tests peut prendre énormément de temps, donc par facilité,
je me contente des résultats que j'avais archivé et que tu seras
content de trouver pour en inclure une partie dans ton rapport.

Tout d'abord un jeu de répertoire de test en instance d'être lancé. Le
répertoire porte le nom de l'image sur laquelle est effectué le
test. On y trouvera l'image originale pleine résolution, et les deux
masques de gradient fin et un peu plus épais. Pour faire le test, il
suffit d'écrire le script comme dans doc/labeling/mp00307.

* doc/labeling/mp00215c
* doc/labeling/mp00234c
* doc/labeling/mp00248c
* doc/labeling/ta00031c
* doc/labeling/ta00083c


Puis, nous avons les tous premiers tests qui ont servi pour déterminer
la quantification à utiliser et la méthode de propagation (infinie ou
saturée). Il y a longtemps, pour ces tests, je disposais d'une chaine
complète mais cette dernière c'est perdu dans la migration entre svn
et git. Il n'y a donc pas de script shell qui lance les outils, par
contre il existe un fichier de synthese qui explique exactement ce qui
a avait été lancé et avec quels paramètres, histoire de pouvoir rejouer le test.

* doc/labeling/cmp_method: Test sur le type de propagation à utiliser.
* doc/labelling/cmp_quant: Test sur la quantification a utiliser.


Puis, nous avons les tests effectués à l'aide d'un script. Lire les
scripts pour situer correctement les binaires outils utilisés. Le
premier jeu test avait pour but d'étudier la combinatoire des options
et de choisir ensuite les paramètres qui vont bien. Il n'y a pas de
certitude complète en la matière. Lire attentivement le fichier
synthèse. Tout ces éléments ont été validés par théo. Je ne me rapelle
plus du détail, mais j'ai rédigé tout ce que je pouvais. Le dernier
répertoire n'a pas de fichier synthèse, je pense que l'étude était du
même gabarit que le test précédent. J'en étais à tester la routine sur
un maximum d'images pour voir si le jeu réduit de couleurs obtenues
correspondait à nos attentes ou non. Les paramètres stables sont à
rechercher dans les derniers répertoires mp00042c et mp00307c.

* doc/labeling/mp00307c_bis: Test sur un grand nombre de combinaisons.
* doc/labeling/mp00411c: Test sur l'altération des statistiques.
* doc/labeling/mp00042c: Test sur la representativité des couleurs trouvées.
* doc/labeling/mp00307c: Test sur la representativité des couleurs trouvées.


Faire le changelog, il y a de l'aide dans git/doc/Changelog
... et commiter


XI ANNOTATING
--------------

==> to do



* doc/annotating



XI AUTRES ASPECTS DOCUMENTAIRES
-------------------------------

Ces exemples de codes sont livrés tels quels sans aucune documentation
de ma part. Si tu as besoin de quelque chose, sert toi, tu les
remanieras à ta sauce. Il n'y a pas de raison de les documenter, se
serait disperser mes efforts vainement.

* doc/examples/accu_color: Petit programme pour se bidouiller avec la couleur.
* doc/examples/frac: Librairie pour manipuler des fractions sous milena.
* doc/examples/hello_milena: Petit exemple allant avec la doc de milena.
* doc/examples/hello_world: Test de la plateforme c++.
* doc/examples/io: Rien de vraiment intéressant dans l'état actuel.
* doc/examples/learn_milena: Autre exemple de base avec milena.
* doc/examples/otsu: Petit programme Otsu.
* doc/examples/stats: Exemple de manipulation d'accumulateurs.


Voici deux exemples sous LaTex. Un jeu de test avec la confection de
vecteur et de matrices pour relater les forrmules connues en espace
3d. La version actuelle est dégradée par rapport à ce que j'avais pu
écrire. J'étais arrivé à la mise au point d'un extracteur de valeurs
propres mais je n'ai jamais pu retrouver cette version là, surement
une erreur sous svn. Le second document devait présenter la
documentation quick tour sous milena, mais j'avoue que c'est un
lamentable échec, je n'ai pas pris le temps de le faire. Je pense que
l'idée d'une documentation collaborative permettrait de répartir
l'effort.

* doc/formulae: LaTex directory.
* doc/quick_tour: LaTex directory.

